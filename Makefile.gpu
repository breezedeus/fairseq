# ein-gpu1
WAV_DIR = /home/ein/jinlong/speech_data/holybell
OUT_DATA_DIR = data
PRETRAIN_MODEL_FP = models/checkpoint_last.pt

gen-manifest:
	python examples/wav2vec/wav2vec_manifest.py $(WAV_DIR) --dest $(OUT_DATA_DIR) --ext wav --valid-percent 0.02

pretrain:
	python train.py $(OUT_DATA_DIR) --save-dir models --task audio_pretraining --criterion wav2vec --arch wav2vec2 \
		--fp16 --num-workers 6 --encoder-layers 4 --update-freq 64 --sample-rate 8000 \
		--log-keys '["prob_perplexity","code_perplexity","temp"]' --quantize-targets --extractor-mode default \
		--conv-feature-layers '[(512, 10, 5)] + [(512, 3, 2)] * 4 + [(512,2,2)] * 2' --final-dim 256 --latent-vars 320 \
		--latent-groups 2 --latent-temp '(2,0.5,0.999995)' --infonce --optimizer adam \
		--adam-betas '(0.9,0.98)' --adam-eps 1e-06 --lr-scheduler polynomial_decay --total-num-update 400000 \
		--lr 0.0005 --warmup-updates 32000 --mask-length 10 --mask-prob 0.65 --mask-selection static --mask-other 0 \
		--encoder-layerdrop 0.05 --dropout-input 0.1 --dropout-features 0.1 --feature-grad-mult 0.1 \
		--loss-weights '[0.1, 10]' --conv-pos 128 --conv-pos-groups 16 --num-negatives 100 --cross-sample-negatives 0 \
		--max-sample-size 160000 --min-sample-size 4000 --dropout 0.1 --attention-dropout 0.1 --weight-decay 0.01 \
		--max-tokens 1400000 --max-update 400000 --skip-invalid-size-inputs-valid-test --ddp-backend no_c10d

prepare-asr:
	python examples/wav2vec/holybell_labels.py $(OUT_DATA_DIR)/train.tsv --label-fp $(WAV_DIR)/all-ori.txt \
		--output-tsv $(OUT_DATA_DIR)/train-clean.tsv --output-dir $(OUT_DATA_DIR) --output-name train
#	mv $(OUT_DATA_DIR)/train-clean.tsv $(OUT_DATA_DIR)/train.tsv
	python examples/wav2vec/holybell_labels.py $(OUT_DATA_DIR)/valid.tsv --label-fp $(WAV_DIR)/all-ori.txt \
		--output-tsv $(OUT_DATA_DIR)/valid-clean.tsv --output-dir $(OUT_DATA_DIR) --output-name valid
#	mv $(OUT_DATA_DIR)/valid-clean.tsv $(OUT_DATA_DIR)/valid.tsv
	python examples/wav2vec/build_vocab.py --base-vocab-fp $(OUT_DATA_DIR)/base_vocab.txt \
		--corpus-text-fp $(WAV_DIR)/train-text.txt -o $(OUT_DATA_DIR)/dict.ltr.txt

finetune-asr:
	python train.py $(OUT_DATA_DIR) --save-dir $(OUT_DATA_DIR)/finetuned-asr \
		--fp16 --num-workers 1 --update-freq 24 --sample-rate 8000 --reset-optimizer \
		 --batch-size 64 --max-sample-size 160000 --min-sample-size 4000 \
		--w2v-path $(PRETRAIN_MODEL_FP) --tensorboard-logdir runs \
		--post-process letter --valid-subset valid --no-epoch-checkpoints --best-checkpoint-metric wer \
		--max-update 80000 --sentence-avg --task audio_pretraining --arch wav2vec_ctc \
		--labels ltr --apply-mask --mask-selection static --mask-other 0 --mask-length 10 --mask-prob 0.5 --layerdrop 0.1 \
		--mask-channel-selection static --mask-channel-other 0 --mask-channel-length 64 --mask-channel-prob 0.5 --zero-infinity \
		--feature-grad-mult 0.0 --freeze-finetune-updates 10000 --validate-after-updates 10000 --optimizer adam \
		--adam-betas '(0.9, 0.98)' --adam-eps 1e-08 --lr 1e-05 --lr-scheduler tri_stage --warmup-steps 8000 --hold-steps 32000 \
		--decay-steps 40000 --final-lr-scale 0.05 --final-dropout 0.0 --dropout 0.0 --activation-dropout 0.1 --criterion ctc \
		--attention-dropout 0.0 --max-tokens 1280000 --seed 2337 --log-format json --log-interval 500 --ddp-backend no_c10d


.PHONY: gen-manifest pretrain prepare-asr finetune-asr
